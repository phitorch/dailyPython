{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNoCkvwUdfO+sQVmTlDzPz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xapxyhgK5ndy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-Or6MWG9c_x",
        "outputId": "c88e2442-fec3-4fc9-8037-2ce019ac966e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb4a40d8e70>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터\n",
        "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
        "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
        "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "metadata": {
        "id": "lUmfzI_W99UY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 w와 편향 b 초기화\n",
        "w1 = torch.zeros(1, requires_grad=True)\n",
        "w2 = torch.zeros(1, requires_grad=True)\n",
        "w3 = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "B2w5OhkI-Ans"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer 설정\n",
        "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTZ7vUZD-EMw",
        "outputId": "6f2edb2b-0f87-4a16-d06c-7601b7c49408"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
            "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
            "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
            "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
            "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
            "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
            "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
            "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
            "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
            "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
            "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  80], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros((3, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1e-5)\n",
        "\n",
        "nb_epochs = 100\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
        "    hypothesis = x_train.matmul(W) + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1HPYrNbJjax",
        "outputId": "63c8547e-d5d7-4bad-ee4f-3fc515323f1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/100 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
            "Epoch    1/100 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n",
            "Epoch    2/100 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7862,  95.8280]) Cost: 3069.590088\n",
            "Epoch    3/100 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n",
            "Epoch    4/100 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481873\n",
            "Epoch    5/100 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n",
            "Epoch    6/100 hypothesis: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]) Cost: 38.687496\n",
            "Epoch    7/100 hypothesis: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]) Cost: 16.499043\n",
            "Epoch    8/100 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]) Cost: 9.365656\n",
            "Epoch    9/100 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]) Cost: 7.071114\n",
            "Epoch   10/100 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331847\n",
            "Epoch   11/100 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n",
            "Epoch   12/100 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]) Cost: 6.013817\n",
            "Epoch   13/100 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986785\n",
            "Epoch   14/100 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]) Cost: 5.976325\n",
            "Epoch   15/100 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971208\n",
            "Epoch   16/100 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967835\n",
            "Epoch   17/100 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]) Cost: 5.964969\n",
            "Epoch   18/100 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962291\n",
            "Epoch   19/100 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959664\n",
            "Epoch   20/100 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]) Cost: 5.957089\n",
            "Epoch   21/100 hypothesis: tensor([154.0543, 185.1140, 175.7470, 198.6150, 141.2166]) Cost: 5.954494\n",
            "Epoch   22/100 hypothesis: tensor([154.0546, 185.1143, 175.7486, 198.6150, 141.2171]) Cost: 5.951927\n",
            "Epoch   23/100 hypothesis: tensor([154.0547, 185.1143, 175.7500, 198.6147, 141.2173]) Cost: 5.949370\n",
            "Epoch   24/100 hypothesis: tensor([154.0546, 185.1142, 175.7512, 198.6143, 141.2175]) Cost: 5.946794\n",
            "Epoch   25/100 hypothesis: tensor([154.0546, 185.1140, 175.7523, 198.6138, 141.2177]) Cost: 5.944210\n",
            "Epoch   26/100 hypothesis: tensor([154.0545, 185.1137, 175.7535, 198.6133, 141.2178]) Cost: 5.941589\n",
            "Epoch   27/100 hypothesis: tensor([154.0543, 185.1135, 175.7545, 198.6127, 141.2178]) Cost: 5.939054\n",
            "Epoch   28/100 hypothesis: tensor([154.0542, 185.1132, 175.7556, 198.6122, 141.2179]) Cost: 5.936491\n",
            "Epoch   29/100 hypothesis: tensor([154.0541, 185.1129, 175.7567, 198.6116, 141.2180]) Cost: 5.933901\n",
            "Epoch   30/100 hypothesis: tensor([154.0539, 185.1126, 175.7578, 198.6110, 141.2180]) Cost: 5.931320\n",
            "Epoch   31/100 hypothesis: tensor([154.0538, 185.1123, 175.7588, 198.6104, 141.2181]) Cost: 5.928727\n",
            "Epoch   32/100 hypothesis: tensor([154.0536, 185.1120, 175.7599, 198.6098, 141.2181]) Cost: 5.926240\n",
            "Epoch   33/100 hypothesis: tensor([154.0535, 185.1118, 175.7610, 198.6093, 141.2182]) Cost: 5.923646\n",
            "Epoch   34/100 hypothesis: tensor([154.0533, 185.1115, 175.7620, 198.6086, 141.2183]) Cost: 5.921031\n",
            "Epoch   35/100 hypothesis: tensor([154.0532, 185.1112, 175.7631, 198.6081, 141.2183]) Cost: 5.918510\n",
            "Epoch   36/100 hypothesis: tensor([154.0530, 185.1109, 175.7641, 198.6075, 141.2184]) Cost: 5.915939\n",
            "Epoch   37/100 hypothesis: tensor([154.0529, 185.1106, 175.7652, 198.6069, 141.2184]) Cost: 5.913393\n",
            "Epoch   38/100 hypothesis: tensor([154.0527, 185.1103, 175.7662, 198.6063, 141.2185]) Cost: 5.910825\n",
            "Epoch   39/100 hypothesis: tensor([154.0526, 185.1100, 175.7673, 198.6057, 141.2185]) Cost: 5.908256\n",
            "Epoch   40/100 hypothesis: tensor([154.0524, 185.1097, 175.7683, 198.6051, 141.2186]) Cost: 5.905738\n",
            "Epoch   41/100 hypothesis: tensor([154.0523, 185.1094, 175.7694, 198.6045, 141.2187]) Cost: 5.903165\n",
            "Epoch   42/100 hypothesis: tensor([154.0521, 185.1091, 175.7705, 198.6039, 141.2187]) Cost: 5.900594\n",
            "Epoch   43/100 hypothesis: tensor([154.0520, 185.1089, 175.7715, 198.6034, 141.2188]) Cost: 5.898070\n",
            "Epoch   44/100 hypothesis: tensor([154.0518, 185.1086, 175.7726, 198.6028, 141.2188]) Cost: 5.895528\n",
            "Epoch   45/100 hypothesis: tensor([154.0517, 185.1083, 175.7736, 198.6022, 141.2189]) Cost: 5.892972\n",
            "Epoch   46/100 hypothesis: tensor([154.0515, 185.1080, 175.7747, 198.6016, 141.2189]) Cost: 5.890395\n",
            "Epoch   47/100 hypothesis: tensor([154.0514, 185.1077, 175.7757, 198.6010, 141.2190]) Cost: 5.887882\n",
            "Epoch   48/100 hypothesis: tensor([154.0512, 185.1074, 175.7768, 198.6004, 141.2191]) Cost: 5.885315\n",
            "Epoch   49/100 hypothesis: tensor([154.0511, 185.1071, 175.7778, 198.5998, 141.2191]) Cost: 5.882764\n",
            "Epoch   50/100 hypothesis: tensor([154.0509, 185.1068, 175.7789, 198.5993, 141.2192]) Cost: 5.880200\n",
            "Epoch   51/100 hypothesis: tensor([154.0508, 185.1065, 175.7800, 198.5987, 141.2192]) Cost: 5.877647\n",
            "Epoch   52/100 hypothesis: tensor([154.0506, 185.1062, 175.7810, 198.5981, 141.2193]) Cost: 5.875116\n",
            "Epoch   53/100 hypothesis: tensor([154.0504, 185.1060, 175.7821, 198.5975, 141.2193]) Cost: 5.872606\n",
            "Epoch   54/100 hypothesis: tensor([154.0503, 185.1057, 175.7831, 198.5969, 141.2194]) Cost: 5.870055\n",
            "Epoch   55/100 hypothesis: tensor([154.0502, 185.1054, 175.7842, 198.5963, 141.2195]) Cost: 5.867533\n",
            "Epoch   56/100 hypothesis: tensor([154.0500, 185.1051, 175.7852, 198.5957, 141.2195]) Cost: 5.864987\n",
            "Epoch   57/100 hypothesis: tensor([154.0498, 185.1048, 175.7863, 198.5952, 141.2196]) Cost: 5.862448\n",
            "Epoch   58/100 hypothesis: tensor([154.0497, 185.1045, 175.7873, 198.5946, 141.2196]) Cost: 5.859921\n",
            "Epoch   59/100 hypothesis: tensor([154.0495, 185.1042, 175.7884, 198.5940, 141.2197]) Cost: 5.857368\n",
            "Epoch   60/100 hypothesis: tensor([154.0494, 185.1039, 175.7894, 198.5934, 141.2198]) Cost: 5.854844\n",
            "Epoch   61/100 hypothesis: tensor([154.0492, 185.1037, 175.7905, 198.5928, 141.2198]) Cost: 5.852332\n",
            "Epoch   62/100 hypothesis: tensor([154.0491, 185.1034, 175.7915, 198.5922, 141.2199]) Cost: 5.849802\n",
            "Epoch   63/100 hypothesis: tensor([154.0489, 185.1031, 175.7926, 198.5916, 141.2199]) Cost: 5.847257\n",
            "Epoch   64/100 hypothesis: tensor([154.0488, 185.1028, 175.7936, 198.5911, 141.2200]) Cost: 5.844723\n",
            "Epoch   65/100 hypothesis: tensor([154.0486, 185.1025, 175.7947, 198.5905, 141.2201]) Cost: 5.842195\n",
            "Epoch   66/100 hypothesis: tensor([154.0485, 185.1022, 175.7957, 198.5899, 141.2201]) Cost: 5.839685\n",
            "Epoch   67/100 hypothesis: tensor([154.0483, 185.1019, 175.7968, 198.5893, 141.2202]) Cost: 5.837132\n",
            "Epoch   68/100 hypothesis: tensor([154.0482, 185.1016, 175.7978, 198.5887, 141.2202]) Cost: 5.834631\n",
            "Epoch   69/100 hypothesis: tensor([154.0480, 185.1013, 175.7989, 198.5882, 141.2203]) Cost: 5.832149\n",
            "Epoch   70/100 hypothesis: tensor([154.0479, 185.1011, 175.7999, 198.5876, 141.2203]) Cost: 5.829581\n",
            "Epoch   71/100 hypothesis: tensor([154.0477, 185.1008, 175.8010, 198.5870, 141.2204]) Cost: 5.827082\n",
            "Epoch   72/100 hypothesis: tensor([154.0476, 185.1005, 175.8020, 198.5864, 141.2205]) Cost: 5.824571\n",
            "Epoch   73/100 hypothesis: tensor([154.0474, 185.1002, 175.8031, 198.5858, 141.2205]) Cost: 5.822036\n",
            "Epoch   74/100 hypothesis: tensor([154.0473, 185.0999, 175.8041, 198.5852, 141.2206]) Cost: 5.819564\n",
            "Epoch   75/100 hypothesis: tensor([154.0471, 185.0996, 175.8051, 198.5846, 141.2206]) Cost: 5.817031\n",
            "Epoch   76/100 hypothesis: tensor([154.0470, 185.0993, 175.8062, 198.5840, 141.2207]) Cost: 5.814490\n",
            "Epoch   77/100 hypothesis: tensor([154.0468, 185.0991, 175.8073, 198.5835, 141.2207]) Cost: 5.811966\n",
            "Epoch   78/100 hypothesis: tensor([154.0466, 185.0988, 175.8083, 198.5829, 141.2208]) Cost: 5.809479\n",
            "Epoch   79/100 hypothesis: tensor([154.0465, 185.0985, 175.8093, 198.5823, 141.2209]) Cost: 5.806978\n",
            "Epoch   80/100 hypothesis: tensor([154.0464, 185.0982, 175.8104, 198.5817, 141.2209]) Cost: 5.804478\n",
            "Epoch   81/100 hypothesis: tensor([154.0462, 185.0979, 175.8114, 198.5812, 141.2210]) Cost: 5.801940\n",
            "Epoch   82/100 hypothesis: tensor([154.0460, 185.0976, 175.8125, 198.5806, 141.2210]) Cost: 5.799462\n",
            "Epoch   83/100 hypothesis: tensor([154.0459, 185.0974, 175.8135, 198.5800, 141.2211]) Cost: 5.796945\n",
            "Epoch   84/100 hypothesis: tensor([154.0457, 185.0971, 175.8146, 198.5794, 141.2212]) Cost: 5.794450\n",
            "Epoch   85/100 hypothesis: tensor([154.0456, 185.0968, 175.8156, 198.5788, 141.2212]) Cost: 5.791949\n",
            "Epoch   86/100 hypothesis: tensor([154.0454, 185.0965, 175.8166, 198.5782, 141.2213]) Cost: 5.789460\n",
            "Epoch   87/100 hypothesis: tensor([154.0453, 185.0962, 175.8177, 198.5777, 141.2213]) Cost: 5.786945\n",
            "Epoch   88/100 hypothesis: tensor([154.0451, 185.0959, 175.8187, 198.5771, 141.2214]) Cost: 5.784457\n",
            "Epoch   89/100 hypothesis: tensor([154.0450, 185.0956, 175.8198, 198.5765, 141.2215]) Cost: 5.781941\n",
            "Epoch   90/100 hypothesis: tensor([154.0448, 185.0954, 175.8208, 198.5759, 141.2215]) Cost: 5.779487\n",
            "Epoch   91/100 hypothesis: tensor([154.0447, 185.0951, 175.8219, 198.5753, 141.2216]) Cost: 5.776975\n",
            "Epoch   92/100 hypothesis: tensor([154.0445, 185.0948, 175.8229, 198.5748, 141.2216]) Cost: 5.774490\n",
            "Epoch   93/100 hypothesis: tensor([154.0444, 185.0945, 175.8239, 198.5742, 141.2217]) Cost: 5.771988\n",
            "Epoch   94/100 hypothesis: tensor([154.0442, 185.0942, 175.8250, 198.5736, 141.2218]) Cost: 5.769504\n",
            "Epoch   95/100 hypothesis: tensor([154.0441, 185.0940, 175.8260, 198.5730, 141.2218]) Cost: 5.767023\n",
            "Epoch   96/100 hypothesis: tensor([154.0439, 185.0937, 175.8271, 198.5724, 141.2219]) Cost: 5.764544\n",
            "Epoch   97/100 hypothesis: tensor([154.0437, 185.0934, 175.8281, 198.5718, 141.2219]) Cost: 5.762030\n",
            "Epoch   98/100 hypothesis: tensor([154.0436, 185.0931, 175.8291, 198.5713, 141.2220]) Cost: 5.759563\n",
            "Epoch   99/100 hypothesis: tensor([154.0434, 185.0928, 175.8302, 198.5707, 141.2221]) Cost: 5.757066\n",
            "Epoch  100/100 hypothesis: tensor([154.0433, 185.0925, 175.8312, 198.5701, 141.2221]) Cost: 5.754573\n"
          ]
        }
      ]
    }
  ]
}