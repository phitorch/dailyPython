{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlSRnqlyOMFMMRITvvR3mP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZC9cZAbmQ2P",
        "outputId": "0ebf2192-bcc8-44e7-f966-160612945c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W = [2.9364457], b = [2.5581357]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 임의의 데이터 생성\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1)\n",
        "y = 2 + 3 * X + np.random.rand(100, 1)\n",
        "\n",
        "# TensorFlow 변수 초기화\n",
        "W = tf.Variable(tf.random.normal([1]))\n",
        "b = tf.Variable(tf.zeros([1]))\n",
        "\n",
        "# 학습률 및 에포크 정의\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# 선형 회귀 모델 정의\n",
        "def linear_regression(x):\n",
        "    return W * x + b\n",
        "\n",
        "# 평균 제곱 오차(MSE) 손실 함수 정의\n",
        "def mean_square(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
        "\n",
        "# 옵티마이저 정의 (경사 하강법 사용)\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(epochs):\n",
        "    # GradientTape를 사용하여 연산을 기록\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = linear_regression(X)\n",
        "        loss = mean_square(y_pred, y)\n",
        "    # 기록된 연산을 사용하여 그래디언트를 계산\n",
        "    gradients = tape.gradient(loss, [W, b])\n",
        "    # 가중치와 편향 업데이트\n",
        "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
        "\n",
        "print(f\"W = {W.numpy()}, b = {b.numpy()}\")\n"
      ]
    }
  ]
}